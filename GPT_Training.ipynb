{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be76a429-5ede-4b03-8dea-b5e3c6c1a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cff02e4-d781-4826-bb44-bb9874b6d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments,\n",
    "    DataCollatorForLanguageModeling, pipeline\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0cb4723-cb41-449e-a78a-89a36073a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 147,456 || all params: 82,060,032 || trainable%: 0.1797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2156: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "base_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd01041b-7c85-4601-855a-d46ffbf4543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"].select(range(500))\n",
    "eval_dataset = tokenized_dataset[\"validation\"].select(range(200))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1840b878-46e9-4570-bbfe-aac81b7e1cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE Fine-tuning ===\n",
      "Artificial intelligence will do a lot more than just provide new ways to connect with online clients. To that end, researchers at Carnegie Mellon University and MIT have identified a new way to connect with an online database of human data. The goal is to create a database of human data that will not only provide new ways to connect with online clients, but will also help with the creation of social networks and artificial intelligence.\n",
      "\n",
      "The researchers propose their research to be published online May 26 in the journal Science Advances.\n",
      "\n",
      "\"The human data that we're trying to create will be a database of human data and will be a database of social networks, social networks, and artificial intelligence,\" says Daniel A. Shiffman, a neuroscientist at Carnegie Mellon who led the research and is one of the authors of the paper. \"We're not just talking about the Internet now; we're talking about the Internet as well.\"\n",
      "\n",
      "The researchers are also developing a way of detecting a behavioral problem by analyzing the human brain's neural network. These networks, which can be understood by analyzing human brain activity, can then help with the creation of social networks and artificial intelligence.\n",
      "\n",
      "Shiffman and his colleagues used a technique known as deep learning, which uses high-level learning, to identify\n"
     ]
    }
   ],
   "source": [
    "vanilla_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "vanilla_pipe = pipeline(\"text-generation\", model=vanilla_model, tokenizer=tokenizer)\n",
    "\n",
    "test_prompt = \"Artificial intelligence will\"\n",
    "print(\"=== BEFORE Fine-tuning ===\")\n",
    "print(vanilla_pipe(test_prompt, max_length=10, num_return_sequences=1)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec70475d-7842-4182-b8da-42aff8c7ea82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 07:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=5.48773974609375, metrics={'train_runtime': 441.601, 'train_samples_per_second': 1.132, 'train_steps_per_second': 0.283, 'total_flos': 8193835008000.0, 'train_loss': 5.48773974609375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    # Lower the learning rate significantly for fine-tuning\n",
    "    learning_rate=3e-5, \n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\",\n",
    "    logging_dir=\"./logs\",\n",
    "    dataloader_pin_memory=False,\n",
    "    # These are needed for Peft\n",
    "    save_safetensors=False,\n",
    "    # This is a new parameter for Peft\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Use the new lora_model in the Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9473ed41-a68f-4cec-b04a-310644349667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt2_fc_only\\\\tokenizer_config.json',\n",
       " './fine_tuned_gpt2_fc_only\\\\special_tokens_map.json',\n",
       " './fine_tuned_gpt2_fc_only\\\\vocab.json',\n",
       " './fine_tuned_gpt2_fc_only\\\\merges.txt',\n",
       " './fine_tuned_gpt2_fc_only\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./fine_tuned_gpt2_fc_only\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gpt2_fc_only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de1bdf0a-3101-48d2-afc2-57dc91df5144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt2_merged\\\\tokenizer_config.json',\n",
       " './fine_tuned_gpt2_merged\\\\special_tokens_map.json',\n",
       " './fine_tuned_gpt2_merged\\\\vocab.json',\n",
       " './fine_tuned_gpt2_merged\\\\merges.txt',\n",
       " './fine_tuned_gpt2_merged\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./fine_tuned_gpt2_lora\")\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./fine_tuned_gpt2_merged\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gpt2_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26dbc051-d09f-4544-b9f7-bafe8af374a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f403a0c2e70847a68d2bd9ec4aa6a8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9822bef1c0124aab95f54e66b8dad490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pruth\\.cache\\huggingface\\hub\\models--Pruthvi-1029--fine-tuned-distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Pruthvi-1029/fine-tuned-distilgpt2/commit/d4391f2a4b56b1eeec2fe1408d50feb658ac4c28', commit_message='Upload tokenizer', commit_description='', oid='d4391f2a4b56b1eeec2fe1408d50feb658ac4c28', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Pruthvi-1029/fine-tuned-distilgpt2', endpoint='https://huggingface.co', repo_type='model', repo_id='Pruthvi-1029/fine-tuned-distilgpt2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a repository name. Replace 'your-username' with your Hugging Face username.\n",
    "repo_name = \"Pruthvi-1029/fine-tuned-distilgpt2\"\n",
    "\n",
    "# Load the merged model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2_merged\")\n",
    "merged_model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2_merged\")\n",
    "\n",
    "# Push the model and tokenizer to the hub\n",
    "merged_model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82984d9-c1c5-44d5-b26d-5f9158047b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
